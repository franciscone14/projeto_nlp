{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something\n",
      "Time over, thanks\n",
      "TEXT: O Lucas Ã© um c****\n"
     ]
    }
   ],
   "source": [
    "with sr.Microphone() as source:\n",
    "    print(\"Say something\")\n",
    "    audio = r.listen(source)\n",
    "    print(\"Time over, thanks\")\n",
    "\n",
    "try:\n",
    "    print(\"TEXT: \" + r.recognize_google(audio, language=\"pt-bt\",)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables\n",
    "auth_url = \"https://api.hatebase.org/4-2/authenticate\"\n",
    "vocab_url = \"https://api.hatebase.org/4-2/get_vocabulary\"\n",
    "lang = \"por\"\n",
    "resp_format = \"json\"\n",
    "token = \"\"\n",
    "pages = 0\n",
    "total_entries = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize authentication payload\n",
    "auth_payload = \"api_key=ZJmEGTDmnEUvkZVMZHNQcgwzRxuMZaNc\"\n",
    "headers = {\n",
    "    'Content-Type': \"application/x-www-form-urlencoded\",\n",
    "    'cache-control': \"no-cache\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# authenticate\n",
    "auth_resp = requests.request(\"POST\", auth_url, data=auth_payload, headers=headers)\n",
    "print(auth_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"datetime\": \"2019-06-19 03:35:32\",\n",
      "    \"terms\": \"Your use of the Hatebase API acknowledges your consent with Hatebase's Terms of Use (hatebase.org\\/terms) and prohibits redistribution of the data herein to any third-parties for any purpose, including but not limited to republication in any form, such as in public code repositories.\",\n",
      "    \"api_key\": \"ZJmEGTDmnEUvkZVMZHNQcgwzRxuMZaNc\",\n",
      "    \"result\": {\n",
      "        \"token\": \"iDhsNBXKbVgMfMryTHGKDqETHwTAuNhB\",\n",
      "        \"expires_on\": \"2019-06-19 04:35:32\"\n",
      "    }\n",
      "} iDhsNBXKbVgMfMryTHGKDqETHwTAuNhB\n"
     ]
    }
   ],
   "source": [
    "# get the session token\n",
    "token = auth_resp.json()[\"result\"][\"token\"]\n",
    "# quick check if it worked\n",
    "print(auth_resp.text, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize vocabulary payload\n",
    "# first without any given page-number\n",
    "vocab_payload = \"token=\" + token + \"&format=\" + resp_format + \"&language=\" + lang\n",
    "voc_resp = requests.request(\"POST\", vocab_url, data=vocab_payload, headers=headers)\n",
    "voc_json = voc_resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many pages in total? and how many results= entries in the vocab?\n",
    "pages = voc_json[\"number_of_pages\"]\n",
    "results = voc_json[\"number_of_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary df from first resultset\n",
    "df_voc = pd.DataFrame(voc_json[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get results of all remaining pages and append to df_voc\n",
    "for page in range(2,pages+1):\n",
    "    vocab_payload = \"token=\" + token + \"&format=json&language=\" + lang + \"&page=\" + str(page)\n",
    "    voc_resp = requests.request(\"POST\", vocab_url, data=vocab_payload, headers=headers)\n",
    "    voc_json = voc_resp.json()\n",
    "    df_voc = df_voc.append(voc_json[\"result\"])\n",
    "\n",
    "# reset df_voc index\n",
    "df_voc.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voc.to_csv(\"hatebase_vocab.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
